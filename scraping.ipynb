{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9MrIm49WBr/CUOIQz3b5Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vai-sys/3D-website/blob/main/scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsKrEpvftZ37",
        "outputId": "8a25d746-265b-416e-8032-b666ca470993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# URL of the website to scrape\n",
        "url = \"https://thehackernews.com\"  # Use the main URL of the website you want to scrape\n",
        "\n",
        "# Fetch the page content\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    # Parse the content with BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # List to store the scraped data\n",
        "    articles = []\n",
        "\n",
        "    # Find all article containers (assuming 'body-post clear' class wraps each post)\n",
        "    for post in soup.find_all('div', class_='body-post clear'):\n",
        "        # Extract the article title\n",
        "        title_tag = post.find('h2', class_='home-title')\n",
        "        title = title_tag.text.strip() if title_tag else 'No Title'\n",
        "\n",
        "        # Extract the article link\n",
        "        link_tag = post.find('a', class_='story-link')\n",
        "        link = link_tag['href'] if link_tag else 'No Link'\n",
        "\n",
        "        # Extract the tags\n",
        "        tags_span = post.find('span', class_='h-tags')\n",
        "        tags = tags_span.text.strip() if tags_span else 'No Tags'\n",
        "\n",
        "        # Extract the description\n",
        "        desc_div = post.find('div', class_='home-desc')\n",
        "        description = desc_div.text.strip() if desc_div else 'No Description'\n",
        "\n",
        "        # Store each article's information as a dictionary\n",
        "        article_info = {\n",
        "            'title': title,\n",
        "            'link': link,\n",
        "            'tags': tags,\n",
        "            'description': description\n",
        "        }\n",
        "\n",
        "        # Append to the articles list\n",
        "        articles.append(article_info)\n",
        "\n",
        "    # Save the data to a JSON file\n",
        "    with open('articles.json', 'w') as json_file:\n",
        "        json.dump(articles, json_file, indent=4)\n",
        "\n",
        "    print(\"Data has been scraped and saved to articles.json\")\n",
        "else:\n",
        "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcGbO1W8tiLY",
        "outputId": "26b023e6-5416-42df-bb87-06f1ff862429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been scraped and saved to articles.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the URL\n",
        "url = \"https://thehackernews.com/\"\n",
        "\n",
        "# Send a request to the website\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Raise an error if request failed\n",
        "\n",
        "# Parse the HTML content with BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Prepare a list to store article data\n",
        "articles_data = []\n",
        "\n",
        "# Find articles on the main page\n",
        "articles = soup.find_all('div', class_='body-post clear')\n",
        "\n",
        "for article in articles:\n",
        "    # Extract title\n",
        "    title_tag = article.find('h2', class_='home-title')\n",
        "    title = title_tag.text if title_tag else \"Unknown\"\n",
        "\n",
        "    # Extract URL\n",
        "    url_tag = article.find('a', class_='story-link')\n",
        "    article_url = url_tag['href'] if url_tag else \"Unknown\"\n",
        "\n",
        "    # Extract description\n",
        "    description_tag = article.find('div', class_='home-desc')\n",
        "    description = description_tag.text.strip() if description_tag else \"Description not available\"\n",
        "\n",
        "    # Extract tags (if available)\n",
        "    tags = []\n",
        "    tags_tag = article.find('span', class_='h-tags')\n",
        "    if tags_tag:\n",
        "        tags = [tag.strip() for tag in tags_tag.text.split('/')]\n",
        "\n",
        "    # Extract publication date (if available)\n",
        "    date_tag = article.find('span', class_='h-datetime')\n",
        "    date_str = date_tag.text.strip() if date_tag else \"Unknown\"\n",
        "    try:\n",
        "        date = datetime.strptime(date_str, '%b %d, %Y').strftime('%Y-%m-%d')\n",
        "    except:\n",
        "        date = \"Unknown\"\n",
        "\n",
        "    # Define incident type based on tags (example categorization)\n",
        "    incident_type = \"Unknown\"\n",
        "    if \"Ransomware\" in tags:\n",
        "        incident_type = \"Ransomware\"\n",
        "    elif \"Phishing\" in tags:\n",
        "        incident_type = \"Phishing\"\n",
        "    # Additional rules can be added for other incident types\n",
        "\n",
        "    # Append the collected data to articles_data list\n",
        "    articles_data.append({\n",
        "        \"title\": title,\n",
        "        \"date\": date,\n",
        "        \"tags\": tags,\n",
        "        \"description\": description,\n",
        "        \"url\": article_url,\n",
        "        \"incident_type\": incident_type,\n",
        "        \"threat_actor\": \"Unknown\"  # This would likely require further analysis\n",
        "    })\n",
        "\n",
        "# Print the scraped data in JSON format\n",
        "import json\n",
        "print(json.dumps(articles_data, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiotvfqZtmrx",
        "outputId": "55750003-f2d2-46f9-e47e-9e174841cb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"title\": \"North Korean Hackers Target Crypto Firms with Hidden Risk Malware on macOS\",\n",
            "        \"date\": \"Unknown\",\n",
            "        \"tags\": [\n",
            "            \"Cryptocurrency\",\n",
            "            \"Malware\"\n",
            "        ],\n",
            "        \"description\": \"A threat actor with ties to the Democratic People's Republic of Korea (DPRK) has been observed targeting cryptocurrency-related businesses with a multi-stage malware capable of infecting Apple macOS devices .  Cybersecurity company SentinelOne, which dubbed the campaign Hidden Risk , attributed it with high confidence to BlueNoroff, which has been previously linked to malware families such as RustBucket , KANDYKORN , ObjCShellz , RustDoor  (aka Thiefbucket ), and TodoSwift .  The activity \\\"uses emails propagating fake news about cryptocurrency trends to infect targets via a malicious application disguised as a PDF file,\\\" researchers Raffaele Sabato, Phil Stokes, and Tom Hegel said  in a report shared with The Hacker News.  \\\"The campaign likely began as early as July 2024 and uses email and PDF lures with fake news headlines or stories about crypto-related topics.\\\"   As revealed  by the U.S. Federal Bureau of Investigation (FBI) in a September 2024 advisory, the\",\n",
            "        \"url\": \"https://thehackernews.com/2024/11/north-korean-hackers-target-crypto.html\",\n",
            "        \"incident_type\": \"Unknown\",\n",
            "        \"threat_actor\": \"Unknown\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"A Hacker's Guide to Password Cracking\",\n",
            "        \"date\": \"Unknown\",\n",
            "        \"tags\": [\n",
            "            \"Password Security\",\n",
            "            \"Network Security\"\n",
            "        ],\n",
            "        \"description\": \"Defending your organization's security is like fortifying a castle\\u2014you need to understand where attackers will strike and how they'll try to breach your walls. And hackers are always searching for weaknesses, whether it's a lax password policy or a forgotten backdoor. To build a stronger defense, you must think like a hacker and anticipate their moves. Read on to learn more about hackers' strategies to crack passwords, the vulnerabilities they exploit, and how you can reinforce your defenses to keep them at bay.  Analysis of the worst passwords  Weak, commonly used passwords represent the easiest targets for hackers. Every year, experts provide\\u00a0 lists\\u00a0of the most frequently used passwords , with classics like \\\" 123456 \\\" and \\\" password \\\" appearing year after year. These passwords are the low-hanging fruit of a hacker's attack strategy. Despite years of security warnings, users still use simple, easy-to-remember passwords\\u2014often based on predictable patterns or personal details that\",\n",
            "        \"url\": \"https://thehackernews.com/2024/11/a-hackers-guide-to-password-cracking.html\",\n",
            "        \"incident_type\": \"Unknown\",\n",
            "        \"threat_actor\": \"Unknown\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"5 Most Common Malware Techniques in 2024\",\n",
            "        \"date\": \"Unknown\",\n",
            "        \"tags\": [\n",
            "            \"Malware Analysis\",\n",
            "            \"Windows Security\"\n",
            "        ],\n",
            "        \"description\": \"Tactics, techniques, and procedures (TTPs) form the foundation of modern defense strategies. Unlike indicators of compromise (IOCs), TTPs are more stable, making them a reliable way to identify specific cyber threats. Here are some of the most commonly used techniques, according to ANY.RUN's Q3 2024  report on malware trends, complete with real-world examples.  Disabling of Windows Event Logging (T1562.002)  Disrupting Windows Event Logging helps attackers prevent the system from recording crucial information about their malicious actions.  Without event logs, important details such as login attempts, file modifications, and system changes go unrecorded, leaving security solutions and analysts with incomplete or missing data.  Windows Event Logging can be manipulated in different ways, including by changing registry keys or using commands like \\\"net stop eventlog\\\". Altering group policies is another common method.  Since many detection mechanisms rely on log analysis to identify s\",\n",
            "        \"url\": \"https://thehackernews.com/2024/11/5-most-common-malware-techniques-in-2024.html\",\n",
            "        \"incident_type\": \"Unknown\",\n",
            "        \"threat_actor\": \"Unknown\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"SteelFox and Rhadamanthys Malware Use Copyright Scams, Driver Exploits to Target Victims\",\n",
            "        \"date\": \"Unknown\",\n",
            "        \"tags\": [\n",
            "            \"Cryptocurrency\",\n",
            "            \"Malware\"\n",
            "        ],\n",
            "        \"description\": \"An ongoing phishing campaign is employing copyright infringement-related themes to trick victims into downloading a newer version of the Rhadamanthys information stealer since July 2024.  Cybersecurity firm Check Point is tracking the large-scale campaign under the name CopyRh(ight)adamantys . Targeted regions include the United States, Europe, East Asia, and South America.  \\\"The campaign impersonates dozens of companies, while each email is sent to a specific targeted entity from a different Gmail account, adapting the impersonated company and the language per targeted entity,\\\" the company said  in a technical analysis. \\\"Almost 70% of the impersonated companies are from the Entertainment /Media and Technology/Software sectors.\\\"  The attacks are notable for the deployment of version 0.7 of the Rhadamanthys stealer, which, as detailed  by Recorded Future's Insikt Group early last month, incorporates artificial intelligence (AI) for optical character recognition\",\n",
            "        \"url\": \"https://thehackernews.com/2024/11/steelfox-and-rhadamanthys-malware-use.html\",\n",
            "        \"incident_type\": \"Unknown\",\n",
            "        \"threat_actor\": \"Unknown\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"China-Aligned MirrorFace Hackers Target EU Diplomats with World Expo 2025 Bait\",\n",
            "        \"date\": \"Unknown\",\n",
            "        \"tags\": [\n",
            "            \"Threat Intelligence\",\n",
            "            \"Cyber Espionage\"\n",
            "        ],\n",
            "        \"description\": \"The China-aligned threat actor known as MirrorFace has been observed targeting a diplomatic organization in the European Union, marking the first time the hacking crew has targeted an entity in the region.  \\\"During this attack, the threat actor used as a lure the upcoming World Expo, which will be held in 2025 in Osaka, Japan,\\\" ESET said  in its APT Activity Report for the period April to September 2024.  \\\"This shows that even considering this new geographic targeting, MirrorFace remains focused on Japan and events related to it.\\\"   MirrorFace, also tracked as Earth Kasha , is assessed to be part of an umbrella group known as APT10, which also comprises clusters tracked as Earth Tengshe and Bronze Starlight. It's known for its targeting of Japanese organizations at least since 2019, although a new campaign observed in early 2023 expanded its operations to include Taiwan and India.  Over the years, the hacking crew's malware arsenal has evolved to include ba\",\n",
            "        \"url\": \"https://thehackernews.com/2024/11/china-aligned-mirrorface-hackers-target.html\",\n",
            "        \"incident_type\": \"Unknown\",\n",
            "        \"threat_actor\": \"Unknown\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"Cisco Releases Patch for Critical URWB Vulnerability in Industrial Wireless Systems\",\n",
            "        \"date\": \"Unknown\",\n",
            "        \"tags\": [\n",
            "            \"Vulnerability\",\n",
            "            \"Wireless Technology\"\n",
            "        ],\n",
            "        \"description\": \"Cisco has released security updates to address a maximum severity security flaw impacting Ultra-Reliable Wireless Backhaul ( URWB ) Access Points that could permit unauthenticated, remote attackers to run commands with elevated privileges.  Tracked as CVE-2024-20418  (CVS score: 10.0), the vulnerability has been described as stemming from a lack of input validation to the web-based management interface of the Cisco Unified Industrial Wireless Software.  \\\"An attacker could exploit this vulnerability by sending crafted HTTP requests to the web-based management interface of an affected system,\\\" Cisco said  in an advisory released Wednesday.  \\\"A successful exploit could allow the attacker to execute arbitrary commands with root privileges on the underlying operating system of the affected device.\\\"   The shortcoming impacts following Cisco products in scenarios where the URWB operating mode is enabled -   Catalyst IW9165D Heavy Duty Access Points  Catalyst IW9165E Rugge\",\n",
            "        \"url\": \"https://thehackernews.com/2024/11/cisco-releases-patch-for-critical-urwb.html\",\n",
            "        \"incident_type\": \"Unknown\",\n",
            "        \"threat_actor\": \"Unknown\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"5 SaaS Misconfigurations Leading to Major Fu*%@ Ups\",\n",
            "        \"date\": \"Unknown\",\n",
            "        \"tags\": [\n",
            "            \"SaaS Security\",\n",
            "            \"Insider Threat\"\n",
            "        ],\n",
            "        \"description\": \"With so many SaaS applications, a range of configuration options, API capabilities, endless integrations, and app-to-app connections, the SaaS risk possibilities are endless. Critical organizational assets and data are at risk from malicious actors, data breaches, and insider threats, which pose many challenges for security teams. Misconfigurations are silent killers, leading to major vulnerabilities. So, how can CISOs reduce the noise? What misconfiguration should security teams focus on first? Here are five major SaaS configuration mistakes that can lead to security breaches. #1 Misconfiguration: HelpDesk Admins Have Excessive Privileges Risk: Help desk teams have access to sensitive account management functions making them prime targets for attackers. Attackers can exploit this by convincing help desk personnel to reset MFA for privileged users, gaining unauthorized access to critical systems. Impact: Compromised help desk accounts can lead to unauthorized changes to admin-\",\n",
            "        \"url\": \"https://thehackernews.com/2024/11/5-saas-misconfigurations-leading-to.html\",\n",
            "        \"incident_type\": \"Unknown\",\n",
            "        \"threat_actor\": \"Unknown\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"Malicious PyPI Package 'Fabrice' Found Stealing AWS Keys from Thousands of Developers\",\n",
            "        \"date\": \"Unknown\",\n",
            "        \"tags\": [\n",
            "            \"Vulnerability\",\n",
            "            \"Cloud Security\"\n",
            "        ],\n",
            "        \"description\": \"Cybersecurity researchers have discovered a malicious package on the Python Package Index (PyPI) that has racked up thousands of downloads for over three years while stealthily exfiltrating developers' Amazon Web Services (AWS) credentials.  The package in question is \\\" fabrice ,\\\" which typosquats a popular Python library known as \\\" fabric ,\\\" which is designed to execute shell commands remotely over SSH.\\u00a0  While the legitimate package has over 202 million downloads, its malicious counterpart has been downloaded  more than 37,100 times to date. As of writing, \\\"fabrice\\\" is still available for download from PyPI. It was first published in March 2021.   The typosquatting package is designed to exploit the trust associated with \\\"fabric,\\\" incorporating \\\"payloads that steal credentials, create backdoors, and execute platform-specific scripts,\\\" security firm Socket said .  \\\"Fabrice\\\" is designed to carry out its malicious actions\",\n",
            "        \"url\": \"https://thehackernews.com/2024/11/malicious-pypi-package-fabrice-found.html\",\n",
            "        \"incident_type\": \"Unknown\",\n",
            "        \"threat_actor\": \"Unknown\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"Canada Orders TikTok to Shut Down Canadian Operations Over Security Concerns\",\n",
            "        \"date\": \"Unknown\",\n",
            "        \"tags\": [\n",
            "            \"National Security\",\n",
            "            \"Social Media\"\n",
            "        ],\n",
            "        \"description\": \"The Canadian government on Wednesday ordered ByteDance-owned TikTok to dissolve its operations in the country, citing national security risks, but stopped short of instituting a ban on the popular video-sharing platform.  \\\"The decision was based on the information and evidence collected over the course of the review and on the advice of Canada's security and intelligence community and other government partners,\\\" Fran\\u00e7ois-Philippe Champagne, Minister of Innovation, Science and Industry, said  in a statement.   The government said it does not intend to block Canadians' access to the app itself or curtail their ability to create new content, stating the use of a social media application is a \\\"personal choice.\\\" The use of the app has already been banned  on Canadian government devices since February 2023.  That having said, it urged Canadians to adopt good cyber security practices and assess the possible risks that could arise from using social media platforms,\",\n",
            "        \"url\": \"https://thehackernews.com/2024/11/canada-orders-tiktok-to-shut-down.html\",\n",
            "        \"incident_type\": \"Unknown\",\n",
            "        \"threat_actor\": \"Unknown\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"VEILDrive Attack Exploits Microsoft Services to Evade Detection and Distribute Malware\",\n",
            "        \"date\": \"Unknown\",\n",
            "        \"tags\": [\n",
            "            \"SaaS Security\",\n",
            "            \"Threat Detection\"\n",
            "        ],\n",
            "        \"description\": \"An ongoing threat campaign dubbed VEILDrive has been observed taking advantage of legitimate services from Microsoft, including Teams, SharePoint, Quick Assist, and OneDrive, as part of its modus operandi.  \\\"Leveraging Microsoft SaaS services \\u2014 including Teams, SharePoint, Quick Assist, and OneDrive \\u2014 the attacker exploited the trusted infrastructures of previously compromised organizations to distribute spear-phishing attacks and store malware,\\\" Israeli cybersecurity company Hunters said  in a new report.  \\\"This cloud-centric strategy allowed the threat actor to avoid detection by conventional monitoring systems.\\\"  Hunters said it discovered the campaign in September 2024 after it responded to a cyber incident targeting a critical infrastructure organization in the United States. It did not disclose the name of the company, instead giving it the designation \\\"Org C.\\\"   The activity is believed to have commenced a month prior, with the attack culminating i\",\n",
            "        \"url\": \"https://thehackernews.com/2024/11/veildrive-attack-exploits-microsoft.html\",\n",
            "        \"incident_type\": \"Unknown\",\n",
            "        \"threat_actor\": \"Unknown\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"Winos 4.0 Malware Infects Gamers Through Malicious Game Optimization Apps\",\n",
            "        \"date\": \"Unknown\",\n",
            "        \"tags\": [\n",
            "            \"Malware\",\n",
            "            \"Online Security\"\n",
            "        ],\n",
            "        \"description\": \"Cybersecurity researchers are warning that a command-and-control (C&C) framework called Winos  is being distributed within gaming-related applications like installation tools, speed boosters, and optimization utilities.  \\\"Winos 4.0 is an advanced malicious framework that o\\ufb00ers comprehensive functionality, a stable architecture, and efficient control over numerous online endpoints to execute further actions,\\\" Fortinet FortiGuard Labs said  in a report shared with The Hacker News. \\\"Rebuilt from Gh0st RAT , it includes several modular components, each handling distinct functions.\\\"  Campaigns distributing Winos 4.0 were documented  back in June by Trend Micro and the KnownSec 404 Team. The cybersecurity companies are tracking the activity cluster under the names Void Arachne and Silver Fox.   These attacks have been observed targeting Chinese-speaking users, leveraging black hat Search Engine Optimization (SEO) tactics, social media, and messaging platforms like Te\",\n",
            "        \"url\": \"https://thehackernews.com/2024/11/new-winos-40-malware-infects-gamers.html\",\n",
            "        \"incident_type\": \"Unknown\",\n",
            "        \"threat_actor\": \"Unknown\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from datetime import datetime\n",
        "import nest_asyncio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enable nested asyncio for Google Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "class SecurityNewsScraper:\n",
        "    def _init_(self):\n",
        "        self.urls = [\n",
        "            'https://thehackernews.com/',\n",
        "            'https://www.darkreading.com/latest-news',\n",
        "            'https://www.csoonline.com/in/',\n",
        "        ]\n",
        "\n",
        "    async def fetch_page(self, session, url):\n",
        "        \"\"\"Fetch a single page\"\"\"\n",
        "        try:\n",
        "            async with session.get(url, ssl=True) as response:\n",
        "                if response.status == 200:\n",
        "                    return await response.text()\n",
        "                else:\n",
        "                    print(f\"Error fetching {url}: Status {response.status}\")\n",
        "                    return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def parse_hacker_news(self, soup):\n",
        "        \"\"\"Parse TheHackerNews articles\"\"\"\n",
        "        articles = []\n",
        "        for post in soup.find_all('div', class_='body-post'):\n",
        "            try:\n",
        "                title = post.find('h2', class_='home-title')\n",
        "                date = post.find('div', class_='item-label')\n",
        "                description = post.find('div', class_='home-desc')\n",
        "\n",
        "                articles.append({\n",
        "                    'date': date.text.strip() if date else None,\n",
        "                    'title': title.text.strip() if title else None,\n",
        "                    'description': description.text.strip() if description else None,\n",
        "                    'source': 'TheHackerNews',\n",
        "                    'scraped_at': datetime.now().isoformat()\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing HackerNews article: {str(e)}\")\n",
        "        return articles\n",
        "\n",
        "    def parse_dark_reading(self, soup):\n",
        "        \"\"\"Parse DarkReading articles\"\"\"\n",
        "        articles = []\n",
        "        for post in soup.find_all('div', class_='article-content'):\n",
        "            try:\n",
        "                title = post.find('h3')\n",
        "                date = post.find('time')\n",
        "                description = post.find('p')\n",
        "\n",
        "                articles.append({\n",
        "                    'date': date.text.strip() if date else None,\n",
        "                    'title': title.text.strip() if title else None,\n",
        "                    'description': description.text.strip() if description else None,\n",
        "                    'source': 'DarkReading',\n",
        "                    'scraped_at': datetime.now().isoformat()\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing DarkReading article: {str(e)}\")\n",
        "        return articles\n",
        "\n",
        "    def parse_csoonline(self, soup):\n",
        "        \"\"\"Parse CSOOnline articles\"\"\"\n",
        "        articles = []\n",
        "        for post in soup.find_all('div', class_='article-text'):\n",
        "            try:\n",
        "                title = post.find('h3')\n",
        "                date = post.find('time')\n",
        "                description = post.find('p')\n",
        "\n",
        "                articles.append({\n",
        "                    'date': date.text.strip() if date else None,\n",
        "                    'title': title.text.strip() if title else None,\n",
        "                    'description': description.text.strip() if description else None,\n",
        "                    'source': 'CSOOnline',\n",
        "                    'scraped_at': datetime.now().isoformat()\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing CSOOnline article: {str(e)}\")\n",
        "        return articles\n",
        "\n",
        "    async def scrape_data(self):\n",
        "        \"\"\"Main scraping function\"\"\"\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            # Fetch all pages concurrently\n",
        "            tasks = [self.fetch_page(session, url) for url in self.urls]\n",
        "            pages = await asyncio.gather(*tasks)\n",
        "\n",
        "            all_articles = []\n",
        "\n",
        "            # Parse each page based on its source\n",
        "            for url, page_content in zip(self.urls, pages):\n",
        "                if page_content:\n",
        "                    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "\n",
        "                    if 'thehackernews.com' in url:\n",
        "                        articles = self.parse_hacker_news(soup)\n",
        "                    elif 'darkreading.com' in url:\n",
        "                        articles = self.parse_dark_reading(soup)\n",
        "                    elif 'csoonline.com' in url:\n",
        "                        articles = self.parse_csoonline(soup)\n",
        "                    else:\n",
        "                        articles = []\n",
        "\n",
        "                    all_articles.extend(articles)\n",
        "\n",
        "            return all_articles\n",
        "\n",
        "    def real_time_scraping(self, interval_minutes=10):\n",
        "        \"\"\"Run continuous scraping with specified interval\"\"\"\n",
        "        while True:\n",
        "            try:\n",
        "                print(f\"\\nStarting scraping at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
        "\n",
        "                # Run the async scraping\n",
        "                loop = asyncio.get_event_loop()\n",
        "                articles = loop.run_until_complete(self.scrape_data())\n",
        "\n",
        "                # Convert to DataFrame\n",
        "                df = pd.DataFrame(articles)\n",
        "\n",
        "                # Clean and process the data\n",
        "                df = df.dropna(subset=['title'])  # Remove entries without titles\n",
        "                df = df.drop_duplicates(subset=['title'])  # Remove duplicates\n",
        "\n",
        "                # Save to JSON\n",
        "                filename = f'security_news_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
        "                df.to_json(filename, orient='records', indent=2)\n",
        "                print(f\"Saved {len(df)} articles to {filename}\")\n",
        "\n",
        "                # Display sample of results\n",
        "                print(\"\\nLatest articles:\")\n",
        "                display(df[['source', 'title']].head())\n",
        "\n",
        "                print(f\"\\nWaiting {interval_minutes} minutes before next scrape...\")\n",
        "                time.sleep(interval_minutes * 60)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in scraping cycle: {str(e)}\")\n",
        "                print(\"Waiting 1 minute before retrying...\")\n",
        "                time.sleep(60)\n",
        "\n",
        "# Usage\n",
        "scraper = SecurityNewsScraper()\n",
        "\n",
        "# For a single scrape:\n",
        "\"\"\"\n",
        "loop = asyncio.get_event_loop()\n",
        "articles = loop.run_until_complete(scraper.scrape_data())\n",
        "df = pd.DataFrame(articles)\n",
        "print(f\"Scraped {len(df)} articles\")\n",
        "display(df[['source', 'title']].head())\n",
        "\"\"\"\n",
        "\n",
        "# For continuous scraping (uncomment to run):\n",
        "scraper.real_time_scraping(interval_minutes=10)"
      ],
      "metadata": {
        "id": "bw-kvf2uvdVS",
        "outputId": "e65aa13b-5d27-4515-adab-14dc68cd36b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting scraping at 2024-11-12 12:47:37...\n",
            "Error in scraping cycle: 'SecurityNewsScraper' object has no attribute 'urls'\n",
            "Waiting 1 minute before retrying...\n",
            "\n",
            "Starting scraping at 2024-11-12 12:48:37...\n",
            "Error in scraping cycle: 'SecurityNewsScraper' object has no attribute 'urls'\n",
            "Waiting 1 minute before retrying...\n",
            "\n",
            "Starting scraping at 2024-11-12 12:49:38...\n",
            "Error in scraping cycle: 'SecurityNewsScraper' object has no attribute 'urls'\n",
            "Waiting 1 minute before retrying...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hcTUBbQCaj9u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}